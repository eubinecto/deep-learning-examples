# encoder layer


figures | explanation
--- | --- |
![](http://jalammar.github.io/images/t/the_transformer_3.png)|the high-level look of the transformer
![](http://jalammar.github.io/images/t/The_transformer_encoders_decoders.png)|There are encoding component and decoding component
![](http://jalammar.github.io/images/t/encoder_with_tensors.png)|In encoder, the word in each pos flows through in its own path
![](http://jalammar.github.io/images/t/encoder_with_tensors_2.png)|each word flows throw feed-forward network
![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm.png)|each sublayer includes a layer-normalisation step
![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png)|the residual connection


